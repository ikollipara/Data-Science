import re
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("ggplot")
import nltk
nltk.download("wordnet")
nltk.download("punkt")
nltk.download("omw-1.4")
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud

from sklearn.experimental import enable_halving_search_cv
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, make_scorer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.model_selection import HalvingGridSearchCV, GridSearchCV, train_test_split, cross_val_score


df = pd.read_csv("movie.csv")
print(df.shape)
df.head(5)


df['length'] = df['text'].map(lambda text: len(text))


values = df["label"].value_counts().values
plt.figure(figsize=(8, 6))
sns.barplot(x=["Negative", "Positive"], y=values, alpha=0.9)

plt.title("Distribution of Positive vs. Negative Reviews")
plt.ylabel("Count")
plt.xlabel("Classification")


df.groupby("label").count()


df.groupby("label")["length"].describe()


threshold = 2300
df[df["length"] < threshold].hist("length", by="label", bins=500, figsize=(12, 6))


plt.figure(figsize=(8, 6))
sns.stripplot(y="length", x="label", data=df[df["length"] > threshold])
plt.title("Comparison of long length reviews and their classification")
plt.xlabel("Classification")
df[df["length"] > threshold].groupby("label").count()


print(df[df["length"] > threshold].corr())
print()
print(df[df["length"] > threshold].corr("spearman"))


# sourced from https://www.geeksforgeeks.org/python-remove-punctuation-from-string/

df['text'] = df['text'].apply(lambda text: re.sub(r'[^\w\s]|\d', ' ', text))
df


lemmatizer = WordNetLemmatizer()


get_ipython().run_cell_magic("time", "", """
df['lemmatized_text'] = df['text'].map(lambda text: ' '.join(lemmatizer.lemmatize(w) for w in nltk.word_tokenize(text.lower())))""")


print(f"""
Before Lemmatization:
{df['text'][171]}

After Lemmatization:
{df['lemmatized_text'][171]}
""")


with open("stop-words.txt") as f:
    stop_words = [lemmatizer.lemmatize(w[:-1]) for w in f]


X = np.asarray(df['lemmatized_text'])
y = np.asarray(df['label'])

X_train, X_test, y_train, y_test = train_test_split(X, y)


bernoulliNB_Binary = Pipeline([
    ("vect", CountVectorizer()),
    ("clf", BernoulliNB())
])


get_ipython().run_cell_magic("time", "", """
param_grid = {
    'vect__binary': [True, False],
    'vect__stop_words': [stop_words, 'english', None],
    'vect__ngram_range': [(1, 1), (1, 2), (2, 2), (2, 3)],
    'clf__alpha': [0.1, 1, 1.5, 2.0]
}

clf_bernoulli_grid = GridSearchCV(bernoulliNB_Binary, param_grid, scoring='f1', n_jobs=-1, verbose=1).fit(X_train, y_train)


print(f"Best Score: {clf_bernoulli_grid.best_score_}")

print(f"Optimal Values: {clf_bernoulli_grid.best_params_}\n")""")


get_ipython().run_cell_magic("time", "", """
ideal_bernoulli_binary = Pipeline([
    ("vect", CountVectorizer(binary=True, stop_words=None, strip_accents='ascii', ngram_range=(1,2))),
    ("clf", BernoulliNB(alpha=0.1))
])

ideal_bernoulli_binary.fit(X_train, y_train)

y_pred = ideal_bernoulli_binary.predict(X_test)

print("Confusion Matrix")
print(confusion_matrix(y_test, y_pred))
print()
print("Classification Report")
print(classification_report(y_test, y_pred, target_names=["Negative", "Positive"]))
print()
print(f"Cross Val Score: {cross_val_score(ideal_bernoulli_binary, X, y, n_jobs=-1)}")""")


multinomial_pipeline = Pipeline([
    ("vect", CountVectorizer()),
    ("clf", MultinomialNB())
])


get_ipython().run_cell_magic("time", "", """

param_grid = {
    'vect__binary': [True, False],
    'vect__analyzer': ['word', 'char', 'char_wb'],
    'vect__strip_accents': ['ascii', 'unicode', None],
    'vect__stop_words': [stop_words, 'english', None],
    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],
    'clf__alpha': [0.1, 0.5, 1]
}

clf_multinomial_pipeline = HalvingGridSearchCV(multinomial_pipeline, param_grid, scoring='f1', n_jobs=-1, verbose=1, factor=2).fit(X_train, y_train)

print(f"Best Score: {clf_multinomial_pipeline.best_score_}")

print(f"Optimal Values: {clf_multinomial_pipeline.best_params_}\n")""")



