import re
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("ggplot")
import nltk
nltk.download("wordnet")
nltk.download("punkt")
nltk.download("omw-1.4")
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud

from sklearn.experimental import enable_halving_search_cv
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, make_scorer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.model_selection import HalvingGridSearchCV, GridSearchCV, train_test_split, cross_val_score


df = pd.read_csv("movie.csv")
print(df.shape)
df.head(5)


X = df['text']
y = df['label']


get_ipython().run_cell_magic("time", "", """
lemmatizer = WordNetLemmatizer()

with open("stop-words.txt") as f:
    stop_words = [lemmatizer.lemmatize(w.strip('\n')) for w in f]

def process(review: str):
    lemmatized_review = (lemmatizer.lemmatize(w) for w in nltk.wordpunct_tokenize(review))
    return ' '.join(w for w in lemmatized_review if not w.isdigit())

X = X.map(lambda text: process(text))


binary_vector = CountVectorizer(ngram_range=(1, 2), stop_words=stop_words, binary=True)
count_vector = CountVectorizer(ngram_range=(1, 2), stop_words=stop_words)

X_binary = binary_vector.fit_transform(X)
X_count = binary_vector.fit_transform(X)""")


Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_binary, y)
Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_count, y)


get_ipython().run_cell_magic("time", "", """
param_grid = {"alpha": [0.001, 0.01, 0.1, 0.2, 0.5, 1, 1.5, 2]}

bernoulli_clf = GridSearchCV(BernoulliNB(), param_grid).fit(Xb_train, yb_train)

print(f"Best Score: {bernoulli_clf.best_score_}")

print(f"Optimal Values: {bernoulli_clf.best_params_}\n")

bernoulli_clf = GridSearchCV(BernoulliNB(), param_grid).fit(Xc_train, yc_train)

print(f"Best Score: {bernoulli_clf.best_score_}")

print(f"Optimal Values: {bernoulli_clf.best_params_}\n")""")


ideal_clf_bernoulli = BernoulliNB(alpha=0.1).fit(Xb_train, yb_train)

y_pred = ideal_clf_bernoulli.predict(Xb_test)

print(confusion_matrix(yb_test, y_pred))
print(classification_report(yb_test, y_pred))

ideal_clf_bernoulli = BernoulliNB(alpha=0.2).fit(Xc_train, yc_train)

y_pred = ideal_clf_bernoulli.predict(Xc_test)
print(confusion_matrix(yc_test, y_pred))
print(classification_report(yc_test, y_pred))


get_ipython().run_cell_magic("time", "", """
param_grid = {"alpha": [0.001, 0.01, 0.1, 0.2, 0.5, 1, 1.05, 1.1, 1.5, 2]}

multinomial_clf = GridSearchCV(MultinomialNB(), param_grid).fit(Xb_train, yb_train)

print(f"Best Score: {multinomial_clf.best_score_}")

print(f"Optimal Values: {multinomial_clf.best_params_}\n")

multinomial_clf = GridSearchCV(MultinomialNB(), param_grid).fit(Xc_train, yc_train)

print(f"Best Score: {multinomial_clf.best_score_}")

print(f"Optimal Values: {multinomial_clf.best_params_}\n")""")


ideal_clf_multinomial = MultinomialNB(alpha=1.1).fit(Xb_train, yb_train)

y_pred = ideal_clf_multinomial.predict(Xb_test)

print(confusion_matrix(yb_test, y_pred))
print(classification_report(yb_test, y_pred))

ideal_clf_multinomial = MultinomialNB(alpha=1.5).fit(Xc_train, yc_train)

y_pred = ideal_clf_multinomial.predict(Xc_test)

print(confusion_matrix(yc_test, y_pred))
print(classification_report(yc_test, y_pred))


get_ipython().run_cell_magic("time", "", """
decision_tree_clf = RandomForestClassifier(n_jobs=-1).fit(Xb_train, yb_train)

y_pred = decision_tree_clf.predict(Xb_test)

print(confusion_matrix(yb_test, y_pred))
print(classification_report(yb_test, y_pred))

decision_tree_clf = RandomForestClassifier(n_jobs=-1).fit(Xc_train, yc_train)
y_pred = decision_tree_clf.predict(Xc_test)

print(confusion_matrix(yc_test, y_pred))
print(classification_report(yc_test, y_pred))""")


get_ipython().run_cell_magic("time", "", """
param_grid = {
    "criterion": ["gini", "entropy"],
    "min_samples_split": [2, 3, 4],
    "max_features": ["sqrt", "log2"],
    "max_depth": [None, 30, 40, 50]
}

print("Count Vector")

decision_tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid, n_jobs=-1, verbose=1, scoring='f1').fit(Xc_train, yc_train)

print(decision_tree_clf.best_score_)
print(decision_tree_clf.best_params_)

print("Binary Vector")

decision_tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid, n_jobs=-1, verbose=1, scoring='f1').fit(Xb_train, yb_train)

print(decision_tree_clf.best_score_)
print(decision_tree_clf.best_params_)
""")


get_ipython().run_cell_magic("time", "", """
decision_tree_clf = RandomForestClassifier(criterion="gini", max_depth=40, min_samples_split=2, max_features="sqrt").fit(Xb_train, yb_train)

y_pred = decision_tree_clf.predict(Xb_test)


print(confusion_matrix(yb_test, y_pred))
print(classification_report(yb_test, y_pred))""")



